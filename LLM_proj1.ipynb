{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91dd7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/45/05/2d5c38c7fa1a091a61e018070b6878410d256be577e5154843d339e59b2c/langchain-0.1.9-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.1.9-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/91/ca/7219b838086086972e662c19e908694bdc6744537fb41b70392501b8b5e4/dataclasses_json-0.6.4-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.21 from https://files.pythonhosted.org/packages/95/c4/cb370b12b8e43bd23976acab001e2b3ae2aeae501d012e3fb3043a380e12/langchain_community-0.0.24-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.26 (from langchain)\n",
      "  Obtaining dependency information for langchain-core<0.2,>=0.1.26 from https://files.pythonhosted.org/packages/27/a0/6cd072db5ff0dec6b080575c0d976fc1bf22063d131f038496c650db7b84/langchain_core-0.1.26-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.0 from https://files.pythonhosted.org/packages/7e/fe/71f16938d8615e9eb0e6116ad45b9acd5b495cc0d99607cf97bd9e1b3849/langsmith-0.1.8-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.1.8-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/57/e9/4368d49d3b462da16a3bac976487764a84dd85cef97232c7bd61f5bdedf3/marshmallow-3.20.2-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.5.0)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.26->langchain)\n",
      "  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain)\n",
      "  Obtaining dependency information for orjson<4.0.0,>=3.9.14 from https://files.pythonhosted.org/packages/8c/37/3623de71a63c2182f121d9efba488ad606a9934d2f4ba3df51baf428fe96/orjson-3.9.15-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading orjson-3.9.15-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     -------------------------------------- 50.7/50.7 kB 518.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.1.9-py3-none-any.whl (816 kB)\n",
      "   ---------------------------------------- 817.0/817.0 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
      "   ---------------------------------------- 246.4/246.4 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.1.8-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 62.2/62.2 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 49.4/49.4 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading orjson-3.9.15-cp311-none-win_amd64.whl (136 kB)\n",
      "   ---------------------------------------- 136.0/136.0 kB 1.6 MB/s eta 0:00:00\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, packaging, orjson, jsonpatch, marshmallow, langsmith, langchain-core, dataclasses-json, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.26 langsmith-0.1.8 marshmallow-3.20.2 orjson-3.9.15 packaging-23.1 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "jupyter-console 6.6.3 requires jupyter-core!=5.0.*,>=4.12, but you have jupyter-core 5.0.0 which is incompatible.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n",
      "spyder-kernels 2.4.4 requires ipykernel<7,>=6.23.2; python_version >= \"3\", but you have ipykernel 6.17.1 which is incompatible.\n",
      "spyder-kernels 2.4.4 requires jupyter-client<9,>=7.4.9; python_version >= \"3\", but you have jupyter-client 7.4.7 which is incompatible.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c78db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/26/a1/75474477af2a1dae3a25f80b72bbaf20e8296191ece7fff2f67984206f33/openai-1.12.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bm\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\bm\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/2c/93/13f25f2f78646bab97aee7680821e30bd85b2ff0fc45d5fdf5393b79716d/httpcore-1.0.4-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\bm\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 226.7/226.7 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 75.6/75.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 77.8/77.8 kB ? eta 0:00:00\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, distro, httpcore, httpx, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4249e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e2204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8d230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b854f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(temperature=0.6)  #one more parameter, model = choose from openai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a13bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"What is the capital of India?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64865ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bm\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opensource library--hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b9cc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\bm\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bm\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bm\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\bm\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\bm\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bm\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa2bc906",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ed158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain is like a wrapper can call open ai llm models, hugging face llm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cbe69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb744a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_huggingface=HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d80320d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"Can you write a poem about world peace?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65739286",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mllm_huggingface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1032\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1031\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[1;32m-> 1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:991\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m   1001\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:741\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m         )\n\u001b[0;32m    728\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    729\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    730\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m         )\n\u001b[0;32m    740\u001b[0m     ]\n\u001b[1;32m--> 741\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:605\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    604\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    606\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:592\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    584\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 592\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    601\u001b[0m         )\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1177\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1176\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1177\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1180\u001b[0m     )\n\u001b[0;32m   1181\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:138\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    135\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    136\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: parameters}, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\n\u001b[0;32m    137\u001b[0m )\n\u001b[1;32m--> 138\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m())\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Response' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "output=llm_huggingface.predict(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66924c9f",
   "metadata": {},
   "source": [
    "# prompt template and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "619922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09285fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of this India?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template=PromptTemplate(input_variables=['country'],\n",
    "                              template=\"Tell me the capital of this {country}?\")\n",
    "\n",
    "prompt_template.format(country=\"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d57d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm.predict(prompt_template=prompt_template, text=[\"India\"])--- won't work use chains instead\n",
    "\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "371d7494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "chain=LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "print(chain.run(\"Italy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b524f2",
   "metadata": {},
   "source": [
    "# combining Multiple Chains using simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2fe813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt=PromptTemplate(input_variables=['country'],\n",
    "                             template=\"What is the capital of this {country}?\")\n",
    "\n",
    "capital_chain=LLMChain(llm=llm, prompt=capital_prompt)\n",
    "\n",
    "famous_template=PromptTemplate(input_variables=['capital'],\n",
    "                              template=\"suggest some amazing places to visit in {capital}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "426e2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_chain=LLMChain(llm=llm, prompt=famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49a5e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a7117db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are some amazing places to visit in New Delhi:\\n\\n1. Red Fort - A magnificent fort built by Mughal Emperor Shah Jahan, it is a UNESCO World Heritage Site and a must-visit for its beautiful architecture and historical significance.\\n\\n2. India Gate - A war memorial dedicated to the Indian soldiers who lost their lives in World War I, it is a popular spot for picnics and evening strolls.\\n\\n3. Qutub Minar - Another UNESCO World Heritage Site, this 73-meter tall minaret is a symbol of Delhi's rich history and is surrounded by beautiful gardens.\\n\\n4. Lotus Temple - A beautiful Bahá'í House of Worship, its unique lotus-shaped architecture and tranquil atmosphere make it a popular spot for meditation and reflection.\\n\\n5. Humayun's Tomb - Built in the 16th century, this stunning tomb is the final resting place of Mughal Emperor Humayun and is known for its intricate design and beautiful gardens.\\n\\n6. Chandni Chowk - One of the oldest and busiest markets in Delhi, this bustling area is a shopper's paradise and a great place to experience the vibrant culture of the city.\\n\\n7. Akshardham Temple - A modern Hindu temple known for its stunning architecture, intricate car\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=SimpleSequentialChain(chains=[capital_chain,famous_chain])\n",
    "chain.run(\"India\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a8578",
   "metadata": {},
   "source": [
    "# sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b27ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25c33074",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt=PromptTemplate(input_variables=['country'],\n",
    "                             template=\"What is the capital of this {country}?\")\n",
    "\n",
    "capital_chain=LLMChain(llm=llm, prompt=capital_prompt, output_key=\"capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ee093c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template=PromptTemplate(input_variables=['capital'],\n",
    "                              template=\"suggest some amazing places to visit in {capital}\")\n",
    "famous_chain=LLMChain(llm=llm, prompt=famous_template,output_key=\"Places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5c38b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8de82f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= SequentialChain(chains=[capital_chain,famous_chain],input_variables=['country'],\n",
    "                      output_variables=['capital','Places'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da7147a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'United States of America', 'capital': '\\n\\nThe capital of the United States of America is Washington, D.C.', 'Places': ' Here are some amazing places to visit in Washington, D.C.:\\n\\n1. The White House: This iconic building is the official residence and workplace of the President of the United States. Visitors can take a tour of the public rooms and learn about the history and significance of this historic building.\\n\\n2. The National Mall: This open park area is home to several iconic monuments and memorials, including the Lincoln Memorial, Washington Monument, and Vietnam Veterans Memorial. It is a must-visit for anyone interested in American history and politics.\\n\\n3. Smithsonian Museums: Washington, D.C. is home to several world-renowned Smithsonian museums, including the National Air and Space Museum, National Museum of Natural History, and National Gallery of Art. Admission to these museums is free, making them a great option for budget travelers.\\n\\n4. Georgetown: This charming neighborhood is known for its historic architecture, cobblestone streets, and upscale shopping and dining options. It is a great place to explore on foot and soak in the local culture.\\n\\n5. The National Zoo: Located in the heart of the city, the National Zoo is home to over 2,000 animals from around the world, including giant pandas, lions, and elephants. It is a popular attraction for families and animal lovers.\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(chain({'country':'United States of America'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9042f69",
   "metadata": {},
   "source": [
    "# Chat models with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a712e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5f0693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b32eb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm=ChatOpenAI(temperature=0.6,model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "206066a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000018508FBFC10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000018509143D90>, temperature=0.6, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35ea2903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why did the AI break up with its computer programmer? It couldn\\'t handle the codependency!\"\\n2. \"I asked my AI assistant for a joke, and it replied, \\'I\\'m sorry, Dave, I\\'m afraid I can\\'t do that.\\'\"\\n3. \"What do you call an AI that loves to dance? A byte-sized disco bot!\"\\n4. \"Why did the AI go to therapy? It had too many unresolved loops!\"\\n5. \"I told my AI assistant to make me laugh, and it responded, \\'I\\'m sorry, I\\'m not programmed for stand-up comedy. But have you heard about the robot who walked into a bar?\\'\"\\n6. \"Why did the AI cross the road? To optimize its pathfinding algorithm!\"\\n7. \"I asked my AI to tell me a joke, and it said, \\'I can\\'t, I\\'m too busy processing dad jokes.\\'\"\\n8. \"Why did the AI go to school? To improve its artificial intelligence!\"\\n9. \"I tried to teach my AI some puns, but it just replied, \\'Error: Sense of humor not found.\\'\"\\n10. \"What do you call an AI that\\'s always late? A procrastinating processor!\"')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#information to give to chatllm model\n",
    "\n",
    "chatllm([\n",
    "    SystemMessage(content=\"You are a comedian AI assistant\"),\n",
    "    HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdc49b",
   "metadata": {},
   "source": [
    "# prompt template+ llm+ output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e2d3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71cf3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "987e5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import  BaseOutputParser            #to modify the output of a llm model beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ed7168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseparatedoutput(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "769ab4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system message\n",
    "\n",
    "template= \"You are a helpful assistant. When user gives any input, you should generate 5 words in a comma separated list\"\n",
    "\n",
    "human_template=\"{text}\"\n",
    "\n",
    "chatprompt= ChatPromptTemplate.from_messages([(\"system\",template),\n",
    "                                              (\"human\",human_template)\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "953677bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain up everything\n",
    "\n",
    "chain= chatprompt|chatllm|Commaseparatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71172bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' bright', ' clever', ' sharp', ' knowledgeable']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"intelligent\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
